"""
Text2Loc Visionary API

æä¾›å¢å¼ºç‰ˆText2Locçš„REST APIæ¥å£
åŒ…å«qwen3-vl:2bè‡ªç„¶è¯­è¨€ç†è§£é›†æˆ
"""

import sys
import os
import logging
import json
import time as time_module
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥é«˜çº§NLUè§£æå™¨
try:
    from enhancements.advanced_nlu import get_advanced_nlu_parser, NLUResult
    ADVANCED_NLU_AVAILABLE = True
except ImportError:
    ADVANCED_NLU_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("é«˜çº§NLUè§£æå™¨ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€è§£æ")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ConversationSession:
    """å¯¹è¯ä¼šè¯ç®¡ç†"""
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.history = []  # å¯¹è¯å†å² [(query, parsed_result), ...]
        self.created_at = datetime.now()
        self.last_active = datetime.now()
    
    def add_turn(self, query: str, parsed_result: Dict[str, Any]):
        """æ·»åŠ ä¸€è½®å¯¹è¯"""
        self.history.append({
            "query": query,
            "parsed_result": parsed_result,
            "timestamp": datetime.now().isoformat()
        })
        self.last_active = datetime.now()
    
    def get_context(self) -> str:
        """è·å–å¯¹è¯ä¸Šä¸‹æ–‡ç”¨äºå¢å¼ºç†è§£"""
        if not self.history:
            return ""
        
        context_parts = []
        for i, turn in enumerate(self.history[-5:], 1):  # ä¿ç•™æœ€è¿‘5è½®
            context_parts.append(f"ç¬¬{i}è½®: {turn['query']}")
        
        return "\n".join(context_parts)
    
    def get_combined_query(self, current_query: str) -> str:
        """å°†å†å²æŸ¥è¯¢ä¸å½“å‰æŸ¥è¯¢åˆå¹¶"""
        if not self.history:
            return current_query
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = self.get_context()
        return f"ã€å¯¹è¯å†å²ã€‘\n{context}\n\nã€å½“å‰æŸ¥è¯¢ã€‘\n{current_query}"


class SessionManager:
    """ä¼šè¯ç®¡ç†å™¨"""
    def __init__(self, max_sessions: int = 100, ttl_minutes: int = 30):
        self.sessions: Dict[str, ConversationSession] = {}
        self.max_sessions = max_sessions
        self.ttl_minutes = ttl_minutes
    
    def get_or_create(self, session_id: Optional[str]) -> tuple[str, ConversationSession]:
        """è·å–æˆ–åˆ›å»ºä¼šè¯"""
        if session_id and session_id in self.sessions:
            return session_id, self.sessions[session_id]
        
        # åˆ›å»ºæ–°ä¼šè¯
        new_session_id = session_id or f"sess_{int(time_module.time() * 1000)}"
        session = ConversationSession(new_session_id)
        self.sessions[new_session_id] = session
        
        # æ¸…ç†è¿‡æœŸä¼šè¯
        self._cleanup_expired()
        
        return new_session_id, session
    
    def _cleanup_expired(self):
        """æ¸…ç†è¿‡æœŸä¼šè¯"""
        now = datetime.now()
        expired_ids = []
        
        for sid, session in self.sessions.items():
            elapsed = (now - session.last_active).total_seconds() / 60
            if elapsed > self.ttl_minutes:
                expired_ids.append(sid)
        
        for sid in expired_ids:
            del self.sessions[sid]
            logger.info(f"ğŸ—‘ï¸ æ¸…ç†è¿‡æœŸä¼šè¯: {sid}")
        
        # å¦‚æœä¼šè¯æ•°è¶…è¿‡ä¸Šé™ï¼Œæ¸…ç†æœ€æ—©çš„
        if len(self.sessions) > self.max_sessions:
            sorted_sessions = sorted(
                self.sessions.items(),
                key=lambda x: x[1].last_active
            )
            for sid, _ in sorted_sessions[:len(sorted_sessions) - self.max_sessions]:
                del self.sessions[sid]
                logger.info(f"ğŸ—‘ï¸ æ¸…ç†æ—§ä¼šè¯: {sid}")


# å…¨å±€ä¼šè¯ç®¡ç†å™¨
session_manager = SessionManager()


@dataclass
class QueryRequest:
    """æŸ¥è¯¢è¯·æ±‚"""
    query: str  # è‡ªç„¶è¯­è¨€æŸ¥è¯¢
    top_k: int = 5  # è¿”å›top-kç»“æœ
    enable_enhanced: bool = True  # æ˜¯å¦ä½¿ç”¨å¢å¼ºåŠŸèƒ½
    return_debug_info: bool = False  # æ˜¯å¦è¿”å›è°ƒè¯•ä¿¡æ¯
    session_id: Optional[str] = None  # ä¼šè¯IDï¼Œç”¨äºäº¤äº’å¼æŸ¥è¯¢
    interactive: bool = True  # æ˜¯å¦å¯ç”¨äº¤äº’å¼æ¨¡å¼

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class DirectionInfo:
    """æ–¹å‘ä¿¡æ¯"""
    direction: str  # æ–¹å‘æè¿°
    confidence: float  # ç½®ä¿¡åº¦
    normalized_direction: str  # å½’ä¸€åŒ–æ–¹å‘

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class ObjectInfo:
    """å¯¹è±¡ä¿¡æ¯"""
    object_name: str  # å¯¹è±¡åç§°
    confidence: float  # ç½®ä¿¡åº¦
    color: Optional[str] = None  # é¢œè‰²
    color_confidence: Optional[float] = None  # é¢œè‰²ç½®ä¿¡åº¦

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class RetrievalResultItem:
    """æ£€ç´¢ç»“æœé¡¹"""
    rank: int  # æ’å
    cell_id: str  # å•å…ƒæ ¼ID
    score: float  # ç›¸ä¼¼åº¦åˆ†æ•°
    method: str  # æ£€ç´¢æ–¹æ³•
    description: str  # æè¿°
    x: float = 0.0  # Xåæ ‡ï¼ˆ2Då¹³é¢ï¼‰
    y: float = 0.0  # Yåæ ‡ï¼ˆ2Då¹³é¢ï¼‰
    confidence: float = 0.0  # ç½®ä¿¡åº¦
    reference_objects: Optional[List[str]] = None  # å‚è€ƒå¯¹è±¡åˆ—è¡¨

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class ParsingDetails:
    """è§£æè¯¦æƒ…"""
    directions: List[str] = None  # æ–¹å‘åˆ—è¡¨
    colors: List[str] = None  # é¢œè‰²åˆ—è¡¨
    objects: List[str] = None  # å¯¹è±¡åˆ—è¡¨
    distances: List[str] = None  # è·ç¦»åˆ—è¡¨
    landmarks: List[str] = None  # åœ°æ ‡åˆ—è¡¨
    
    def __post_init__(self):
        if self.directions is None:
            self.directions = []
        if self.colors is None:
            self.colors = []
        if self.objects is None:
            self.objects = []
        if self.distances is None:
            self.distances = []
        if self.landmarks is None:
            self.landmarks = []
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class QueryResponse:
    """æŸ¥è¯¢å“åº”"""
    query_id: str  # æŸ¥è¯¢ID
    status: str  # çŠ¶æ€
    processing_time_ms: float  # å¤„ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    
    # è§£æç»“æœ
    query_analysis: Optional[Dict[str, Any]] = None
    parsing_details: Optional[ParsingDetails] = None  # è§£æè¯¦æƒ…
    
    # æ£€ç´¢ç»“æœ
    retrieval_results: Optional[List[RetrievalResultItem]] = None
    results: Optional[List[Dict[str, Any]]] = None  # å…¼å®¹æ—§æ¥å£
    
    # æœ€ç»ˆç»“æœ
    final_result: Optional[RetrievalResultItem] = None
    
    # æ¨¡å¼
    mode: str = "standard"  # è¿è¡Œæ¨¡å¼: standard, enhanced, interactive
    
    # äº¤äº’å¼ä¿¡æ¯
    session_id: Optional[str] = None  # ä¼šè¯ID
    need_clarification: bool = False  # æ˜¯å¦éœ€è¦æ¾„æ¸…
    clarification_question: Optional[str] = None  # æ¾„æ¸…é—®é¢˜
    suggestions: Optional[List[str]] = None  # å»ºè®®
    intent: Optional[str] = None  # æ„å›¾ç±»å‹
    
    # è°ƒè¯•ä¿¡æ¯
    debug_info: Optional[Dict[str, Any]] = None
    
    # é”™è¯¯ä¿¡æ¯
    error: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


class Text2LocAPI:
    """Text2Loc Visionary API"""
    
    def __init__(self, adapter=None, config=None):
        """
        åˆå§‹åŒ–API
        
        Args:
            adapter: Text2Locé€‚é…å™¨å®ä¾‹
            config: é…ç½®
        """
        self.config = config
        self.query_count = 0
        
        # åˆå§‹åŒ– Text2Loc é€‚é…å™¨ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼‰
        if adapter is None:
            from .text2loc_adapter import get_text2loc_adapter
            self.adapter = get_text2loc_adapter()
        else:
            self.adapter = adapter
        
        # åˆå§‹åŒ–NLUå¼•æ“
        self._init_nlu_engine()
        
        logger.info("Text2Loc Visionary APIåˆå§‹åŒ–å®Œæˆ")
    
    def _init_nlu_engine(self):
        """åˆå§‹åŒ–NLUå¼•æ“ï¼ˆæ”¯æŒ DeepSeekã€Ollama å’Œ OpenAI å…¼å®¹æ¥å£ï¼‰"""
        try:
            from .config_api import get_config_manager
            config_manager = get_config_manager()
            model_config = config_manager.get_full_config()
            
            provider = model_config.provider
            logger.info(f"ğŸ”„ åˆå§‹åŒ– NLU å¼•æ“: {provider}")
            
            if provider == "deepseek" and model_config.api_key:
                # DeepSeek é…ç½®
                from enhancements.nlu.deepseek_engine import DeepSeekNLUEngine, DeepSeekConfig
                
                ds_config = DeepSeekConfig(
                    api_key=model_config.api_key,
                    base_url=model_config.base_url or "https://api.deepseek.com",
                    model=model_config.model or "deepseek-chat",
                    enabled=True,
                    timeout=model_config.timeout
                )
                self.nlu_engine = DeepSeekNLUEngine(config=ds_config)
                logger.info(f"âœ… NLUå¼•æ“å·²åˆå§‹åŒ– (DeepSeek)")
                logger.info(f"   æ¨¡å‹: {model_config.model}")
                logger.info(f"   URL: {model_config.base_url}")
                return
                
            elif provider == "ollama":
                # Ollama é…ç½®
                from enhancements.nlu.ollama_engine import OllamaNLUEngine, OllamaConfig
                
                ollama_config = OllamaConfig(
                    base_url=model_config.base_url or "http://localhost:11434",
                    model=model_config.model or "qwen3-vl:2b",
                    enabled=True,
                    timeout=model_config.timeout
                )
                self.nlu_engine = OllamaNLUEngine(config=ollama_config)
                logger.info(f"âœ… NLUå¼•æ“å·²åˆå§‹åŒ– (Ollama)")
                logger.info(f"   æ¨¡å‹: {model_config.model}")
                logger.info(f"   URL: {model_config.base_url}")
                return
                
            elif provider == "openai" and model_config.api_key:
                # OpenAI å…¼å®¹æ¥å£
                from enhancements.nlu.openai_engine import OpenAINLUEngine, OpenAIConfig
                
                openai_config = OpenAIConfig(
                    api_key=model_config.api_key,
                    base_url=model_config.base_url,
                    model=model_config.model,
                    enabled=True,
                    timeout=model_config.timeout
                )
                self.nlu_engine = OpenAINLUEngine(config=openai_config)
                logger.info(f"âœ… NLUå¼•æ“å·²åˆå§‹åŒ– (OpenAI å…¼å®¹)")
                logger.info(f"   æ¨¡å‹: {model_config.model}")
                logger.info(f"   URL: {model_config.base_url}")
                return
                
        except Exception as e:
            logger.warning(f"é…ç½®å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # å›é€€åˆ°ç¯å¢ƒå˜é‡é…ç½®
        deepseek_api_key = os.environ.get("DEEPSEEK_API_KEY", "")
        if deepseek_api_key:
            try:
                from enhancements.nlu.deepseek_engine import DeepSeekNLUEngine, DeepSeekConfig
                
                ds_config = DeepSeekConfig(
                    api_key=deepseek_api_key,
                    base_url=os.environ.get("DEEPSEEK_BASE_URL", "https://api.deepseek.com"),
                    model="deepseek-chat",
                    enabled=True,
                    timeout=30
                )
                self.nlu_engine = DeepSeekNLUEngine(config=ds_config)
                logger.info("âœ… NLUå¼•æ“å·²åˆå§‹åŒ– (DeepSeek - ç¯å¢ƒå˜é‡)")
                return
            except Exception as e:
                logger.warning(f"DeepSeek åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # æœ€ç»ˆå›é€€åˆ°è§„åˆ™è§£æ
        try:
            from enhancements.nlu.optimized_engine import OptimizedNLUEngine, NLUConfig
            nlu_config = NLUConfig(
                model_name="rule-based",
                mock_mode=True,
                enable_dialog=False,
                confidence_threshold=0.6,
                timeout=10
            )
            self.nlu_engine = OptimizedNLUEngine(config=nlu_config)
            logger.info("âœ… NLUå¼•æ“å·²åˆå§‹åŒ– (è§„åˆ™è§£æ)")
        except Exception as e:
            logger.warning(f"âš ï¸ NLUå¼•æ“åˆå§‹åŒ–å¤±è´¥: {str(e)[:50]}")
            self.nlu_engine = None
    
    def set_adapter(self, adapter):
        """è®¾ç½®é€‚é…å™¨"""
        self.adapter = adapter
    
    def _parse_query_with_nlu(self, query: str, session_id: Optional[str] = None, interactive: bool = True) -> Dict[str, Any]:
        """
        ä½¿ç”¨ä¼˜åŒ–ç‰ˆNLUå¼•æ“è§£ææŸ¥è¯¢ï¼ˆæ”¯æŒäº¤äº’å¼ï¼‰
        
        ç­–ç•¥ï¼š
        1. è·å–æˆ–åˆ›å»ºä¼šè¯ï¼Œç»´æŠ¤å¯¹è¯å†å²
        2. å°†å½“å‰æŸ¥è¯¢ä¸ä¼šè¯å†å²åˆå¹¶
        3. ä½¿ç”¨AIæ¨¡å‹è§£æåˆå¹¶åçš„æŸ¥è¯¢
        4. ä¿å­˜è§£æç»“æœåˆ°ä¼šè¯å†å²
        
        Args:
            query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            session_id: ä¼šè¯ID
            interactive: æ˜¯å¦å¯ç”¨äº¤äº’å¼æ¨¡å¼
            
        Returns:
            è§£æç»“æœå­—å…¸
        """
        start_time = time_module.perf_counter()
        
        # è·å–æˆ–åˆ›å»ºä¼šè¯
        session_id, session = session_manager.get_or_create(session_id)
        
        # æ„å»ºå¸¦ä¸Šä¸‹æ–‡çš„æŸ¥è¯¢
        if interactive and session.history:
            contextual_query = session.get_combined_query(query)
            logger.info(f"ğŸ“ ä½¿ç”¨ä¼šè¯ä¸Šä¸‹æ–‡ (å†å²è½®æ•°: {len(session.history)})")
        else:
            contextual_query = query
        
        # æ£€æŸ¥æ˜¯å¦é…ç½®äº† AI æ¨¡å‹ï¼ˆDeepSeek æˆ– Ollamaï¼‰
        deepseek_api_key = os.environ.get("DEEPSEEK_API_KEY", "")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ Ollamaï¼ˆé€šè¿‡é…ç½®æˆ–ç¯å¢ƒå˜é‡ï¼‰
        use_ollama = (
            (self.config and self.config.provider == "ollama") or
            os.environ.get("OLLAMA_URL", "") != "" or
            os.path.exists(os.path.join(os.path.dirname(__file__), '..', 'config', 'model_config.json'))
        )
        
        # å¦‚æœé…ç½®äº† AI æ¨¡å‹ï¼Œä¼˜å…ˆä½¿ç”¨ï¼ˆDeepSeek æˆ– Ollamaï¼‰
        if (deepseek_api_key or use_ollama) and self.nlu_engine is not None:
            try:
                # è·å–æ¨¡å‹åç§°
                if hasattr(self.nlu_engine, 'config') and hasattr(self.nlu_engine.config, 'model'):
                    model_name = self.nlu_engine.config.model
                else:
                    model_name = "deepseek-chat"
                
                logger.info(f"ğŸ¤– ä½¿ç”¨ AI æ¨¡å‹è§£æ: {model_name}")
                nlu_result = self.nlu_engine.parse(contextual_query)
                ai_time = time_module.perf_counter() - start_time
                
                # è°ƒè¯•æ—¥å¿—
                logger.info(f"ğŸ“ NLU åŸå§‹ç»“æœ: {nlu_result}")
                if hasattr(nlu_result, 'components'):
                    logger.info(f"ğŸ“ NLU components: {nlu_result.components}")
                if hasattr(nlu_result, 'confidence'):
                    logger.info(f"ğŸ“ NLU confidence: {nlu_result.confidence}")
                
                # æ£€æŸ¥è§£æç»“æœæ˜¯å¦æœ‰æ•ˆ
                has_error = (nlu_result and 
                    hasattr(nlu_result, 'components') and 
                    isinstance(nlu_result.components, dict) and
                    "error" in nlu_result.components)
                
                if (nlu_result and 
                    hasattr(nlu_result, 'components') and 
                    nlu_result.components and
                    isinstance(nlu_result.components, dict) and
                    not has_error and
                    nlu_result.confidence >= 0):
                    
                    components = nlu_result.components
                    result = {
                        "direction": self._extract_value(components, "direction"),
                        "color": self._extract_value(components, "color"),
                        "object": self._extract_value(components, "object"),
                        "relation": self._extract_value(components, "relation"),
                        "distance": self._extract_value(components, "distance"),
                        "confidence": nlu_result.confidence if hasattr(nlu_result, 'confidence') else 0.85,
                        "enhanced_used": True,
                        "parse_time": ai_time,
                        "nlu_model": model_name,
                        "intent": getattr(nlu_result, 'intent', None),
                        "need_clarification": getattr(nlu_result, 'need_clarification', False),
                        "clarification_question": getattr(nlu_result, 'clarification_question', None),
                        "session_id": session_id,
                        "real_model_used": True,
                        "parse_method": f"ai_{model_name.replace('-', '_')}",
                        "turn_count": len(session.history) + 1
                    }
                    
                    # ä¿å­˜åˆ°ä¼šè¯å†å²
                    session.add_turn(query, result)
                    logger.info(f"âœ… AIæ¨¡å‹è§£ææˆåŠŸ: {model_name}, confidence={result['confidence']:.2f}, è½®æ•°={result['turn_count']}")
                    return result
                else:
                    # AI è§£æå¤±è´¥ï¼Œå›é€€åˆ°è§„åˆ™è§£æ
                    logger.warning(f"âš ï¸ AIæ¨¡å‹è§£æå¤±è´¥ï¼Œå›é€€åˆ°è§„åˆ™è§£æ")
                    simple_result = self._simple_parse(query)
                    simple_result["parse_time"] = time_module.perf_counter() - start_time
                    simple_result["real_model_used"] = False
                    simple_result["parse_method"] = "rule_fallback"
                    simple_result["session_id"] = session_id
                    simple_result["turn_count"] = len(session.history) + 1
                    session.add_turn(query, simple_result)
                    return simple_result
                
            except Exception as e:
                logger.error(f"âŒ AIæ¨¡å‹å¼‚å¸¸: {e}")
                # AI å¼‚å¸¸ï¼Œå›é€€åˆ°è§„åˆ™è§£æ
                simple_result = self._simple_parse(query)
                simple_result["parse_time"] = time_module.perf_counter() - start_time
                simple_result["real_model_used"] = False
                simple_result["parse_method"] = "rule_exception"
                simple_result["session_id"] = session_id
                simple_result["turn_count"] = len(session.history) + 1
                session.add_turn(query, simple_result)
                return simple_result
        
        # æ²¡æœ‰é…ç½® AI æ¨¡å‹ï¼Œä½¿ç”¨è§„åˆ™è§£æ
        simple_result = self._simple_parse(query)
        parse_time = time_module.perf_counter() - start_time
        simple_result["parse_time"] = parse_time
        simple_result["real_model_used"] = False
        simple_result["parse_method"] = "rule_only"
        simple_result["session_id"] = session_id
        simple_result["turn_count"] = len(session.history) + 1
        session.add_turn(query, simple_result)
        logger.info(f"âš ï¸ æœªé…ç½® AI æ¨¡å‹ï¼Œä½¿ç”¨è§„åˆ™è§£æ: confidence={simple_result['confidence']:.2f}")
        return simple_result
    
    def _extract_value(self, components: Dict[str, Any], field: str) -> Any:
        """ä»ç»„ä»¶ä¸­æå–å€¼"""
        if field not in components:
            return None
        
        value = components[field]
        if isinstance(value, dict):
            return value.get("value")
        return value
    
    def _simple_parse(self, query: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨é«˜çº§NLUè§£æå™¨æˆ–å›é€€åˆ°åŸºç¡€è§„åˆ™è§£æ
        
        Args:
            query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            
        Returns:
            è§£æç»“æœå­—å…¸
        """
        start_time = time_module.perf_counter()
        
        # ä¼˜å…ˆä½¿ç”¨é«˜çº§NLUè§£æå™¨
        if ADVANCED_NLU_AVAILABLE:
            try:
                parser = get_advanced_nlu_parser()
                result = parser.parse(query)
                
                parse_time = time_module.perf_counter() - start_time
                
                return {
                    "direction": result.direction,
                    "color": result.color,
                    "object": result.object,
                    "relation": result.relation,
                    "distance": result.distance,
                    "landmarks": result.landmarks,
                    "confidence": result.confidence,
                    "enhanced_used": True,
                    "parse_time": parse_time,
                    "intent": result.intent,
                    "parse_method": "advanced_nlu",
                    "need_clarification": False,
                    "clarification_question": None,
                    "suggestions": []
                }
            except Exception as e:
                logger.warning(f"é«˜çº§NLUè§£æå¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€è§£æ: {e}")
        
        # åŸºç¡€è§„åˆ™è§£æï¼ˆå›é€€æ–¹æ¡ˆï¼‰
        return self._basic_rule_parse(query, start_time)
    
    def _basic_rule_parse(self, query: str, start_time: float) -> Dict[str, Any]:
        """
        åŸºç¡€åŸºäºè§„åˆ™çš„è§£æï¼ˆæœ€ç»ˆå›é€€æ–¹æ¡ˆï¼‰
        
        Args:
            query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            start_time: å¼€å§‹æ—¶é—´
            
        Returns:
            è§£æç»“æœå­—å…¸
        """
        query_lower = query.lower()
        
        # æ–¹å‘è¯†åˆ«ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
        direction_keywords = {
            "north": ["åŒ—", "north", "å‰æ–¹", "å‰ä¾§", "åŒ—ä¾§", "å‰é¢", "å‰æ–¹", "åŒ—è¾¹"],
            "south": ["å—", "south", "åæ–¹", "åä¾§", "å—ä¾§", "åé¢", "åæ–¹", "å—è¾¹"],
            "east": ["ä¸œ", "east", "å³ä¾§", "å³è¾¹", "ä¸œä¾§", "å³é¢", "ä¸œè¾¹", "ä¸œä¾§"],
            "west": ["è¥¿", "west", "å·¦ä¾§", "å·¦è¾¹", "è¥¿ä¾§", "å·¦é¢", "è¥¿è¾¹", "å·¦ä¾§"],
            "northeast": ["ä¸œåŒ—", "northeast", "ä¸œåŒ—æ–¹", "ä¸œåŒ—æ–¹å‘", "ä¸œåŒ—è§’"],
            "northwest": ["è¥¿åŒ—", "northwest", "è¥¿åŒ—æ–¹", "è¥¿åŒ—æ–¹å‘", "è¥¿åŒ—è§’"],
            "southeast": ["ä¸œå—", "southeast", "ä¸œå—æ–¹", "ä¸œå—æ–¹å‘", "ä¸œå—è§’"],
            "southwest": ["è¥¿å—", "southwest", "è¥¿å—æ–¹", "è¥¿å—æ–¹å‘", "è¥¿å—è§’"]
        }
        
        direction = None
        direction_matches = []
        for dir_name, keywords in direction_keywords.items():
            for keyword in keywords:
                if keyword in query:
                    direction = dir_name
                    direction_matches.append(keyword)
                    break
            if direction:
                break
        
        # é¢œè‰²è¯†åˆ«
        color_keywords = {
            "red": ["çº¢", "red", "çº¢è‰²", "çº¢çº¢", "èµ¤è‰²"],
            "blue": ["è“", "blue", "è“è‰²", "è“è“", "å¤©è“"],
            "green": ["ç»¿", "green", "ç»¿è‰²", "ç»¿ç»¿", "è‰ç»¿"],
            "gray": ["ç°", "gray", "ç°è‰²", "ç°ç°", "é“¶ç°"],
            "white": ["ç™½", "white", "ç™½è‰²", "ç™½ç™½", "ä¹³ç™½"],
            "black": ["é»‘", "black", "é»‘è‰²", "é»‘é»‘", "æ¼†é»‘"],
            "yellow": ["é»„", "yellow", "é»„è‰²", "é»„é»„", "é‡‘é»„"],
            "orange": ["æ©™", "orange", "æ©™è‰²", "æ©™æ©™", "æ©˜é»„"]
        }
        
        color = None
        color_matches = []
        for color_name, keywords in color_keywords.items():
            for keyword in keywords:
                if keyword in query:
                    color = color_name
                    color_matches.append(keyword)
                    break
            if color:
                break
        
        # å¯¹è±¡è¯†åˆ«ï¼ˆæ‰©å±•åˆ°22ç§æ ‡å‡†ç±»åˆ«ï¼‰
        object_keywords = {
            "building": ["å¤§æ¥¼", "å»ºç­‘", "building", "å»ºç­‘ç‰©", "é«˜æ¥¼", "æˆ¿å­", "æˆ¿å±‹"],
            "parking": ["åœè½¦", "è½¦ä½", "parking", "åœè½¦åœº", "åœè½¦ä½"],
            "sign": ["æ ‡å¿—", "æ ‡è¯†", "sign", "æŒ‡ç¤ºç‰Œ", "æ ‡ç‰Œ", "äº¤é€šæ ‡å¿—"],
            "light": ["ç¯", "è·¯ç¯", "light", "äº¤é€šç¯", "çº¢ç»¿ç¯", "ä¿¡å·ç¯"],
            "tree": ["æ ‘", "æ ‘æœ¨", "tree", "å¤§æ ‘", "æ ‘æ—", "æ—æœ¨"],
            "car": ["è½¦", "æ±½è½¦", "car", "è½¦è¾†", "æœºåŠ¨è½¦"],
            "pole": ["æŸ±å­", "ç¯æŸ±", "pole", "ç”µçº¿æ†", "æ†å­"],
            "bridge": ["æ¡¥", "æ¡¥æ¢", "bridge", "å¤©æ¡¥"],
            "fence": ["å›´å¢™", "æ …æ ", "fence", "æ æ†"],
            "wall": ["å¢™", "å¢™å£", "wall", "å¢™ä½“"],
            "road": ["é“è·¯", "é©¬è·¯", "road", "å…¬è·¯"],
            "sidewalk": ["äººè¡Œé“", "æ­¥é“", "sidewalk", "ä¾¿é“"],
            "terrain": ["åœ°å½¢", "åœ°é¢", "terrain", "åœŸåœ°"],
            "vegetation": ["æ¤è¢«", "æ¤ç‰©", "vegetation", "è‰æœ¨"],
            "water": ["æ°´", "æ²³æµ", "æ¹–", "water", "river", "lake"],
            "mountain": ["å±±", "å±±å³°", "mountain", "å±±ä¸˜", "ä¸˜é™µ"],
            "rock": ["çŸ³å¤´", "å²©çŸ³", "rock", "çŸ³å—"],
            "path": ["å°è·¯", "è·¯å¾„", "path", "é“è·¯", "å°å¾„"],
            "entrance": ["å…¥å£", "é—¨å£", "entrance", "å¤§é—¨"],
            "corner": ["è§’è½", "æ‹è§’", "corner", "å¢™è§’"],
            "junction": ["è·¯å£", "äº¤å‰å£", "junction", "äº¤æ±‡å¤„"],
            "garage": ["è½¦åº“", "åœè½¦åº“", "garage"],
            "box": ["ç®±å­", "ç›’å­", "box", "æ–¹å—"]
        }
        
        obj = None
        obj_matches = []
        for obj_name, keywords in object_keywords.items():
            for keyword in keywords:
                if keyword in query:
                    obj = obj_name
                    obj_matches.append(keyword)
                    break
            if obj:
                break
        
        # ç©ºé—´å…³ç³»è¯†åˆ«
        relation_keywords = {
            "near": ["é è¿‘", "é‚»è¿‘", "é™„è¿‘", "æ—è¾¹", "è¿‘", "beside", "next to"],
            "between": ["ä¹‹é—´", "ä¸­é—´", "å½“ä¸­", "between"],
            "above": ["ä¸Šæ–¹", "ä¸Šé¢", "é¡¶éƒ¨", "above", "over"],
            "below": ["ä¸‹æ–¹", "ä¸‹é¢", "åº•éƒ¨", "below", "under"],
            "in_front_of": ["å‰é¢", "å‰æ–¹", "æ­£å‰æ–¹", "in front of"],
            "behind": ["åé¢", "åæ–¹", "èƒŒå", "behind"],
            "left_of": ["å·¦è¾¹", "å·¦ä¾§", "left of"],
            "right_of": ["å³è¾¹", "å³ä¾§", "right of"]
        }
        
        relation = None
        relation_matches = []
        for rel_name, keywords in relation_keywords.items():
            for keyword in keywords:
                if keyword in query:
                    relation = rel_name
                    relation_matches.append(keyword)
                    break
            if relation:
                break
        
        # è·ç¦»è¯†åˆ«
        import re
        distance_value = None
        distance_match = None
        
        # æ•°å­—+ç±³æ¨¡å¼
        match = re.search(r'(\d+(?:\.\d+)?)\s*ç±³', query)
        if match:
            try:
                distance_value = float(match.group(1))
                distance_match = f"{distance_value}ç±³"
            except:
                pass
        
        # è®¡ç®—ç½®ä¿¡åº¦ - åŸºäºå®é™…åŒ¹é…æƒ…å†µåŠ¨æ€è®¡ç®—
        confidence_items = []
        
        if direction:
            # æ–¹å‘ç½®ä¿¡åº¦åŸºäºåŒ¹é…å…³é”®è¯çš„é•¿åº¦å’Œå…·ä½“æ€§
            match_len = max([len(m) for m in direction_matches]) if direction_matches else 3
            conf = 0.7 + (match_len * 0.05)  # å…³é”®è¯è¶Šé•¿è¶Šå…·ä½“
            confidence_items.append(("direction", min(conf, 0.95)))
        
        if color:
            match_len = max([len(m) for m in color_matches]) if color_matches else 2
            conf = 0.65 + (match_len * 0.05)
            confidence_items.append(("color", min(conf, 0.90)))
        
        if obj:
            match_len = max([len(m) for m in obj_matches]) if obj_matches else 2
            conf = 0.7 + (match_len * 0.04)
            confidence_items.append(("object", min(conf, 0.95)))
        
        if relation:
            match_len = max([len(m) for m in relation_matches]) if relation_matches else 2
            conf = 0.6 + (match_len * 0.05)
            confidence_items.append(("relation", min(conf, 0.85)))
        
        if distance_value:
            conf = 0.85 if distance_value <= 50 else 0.75  # è¿‘è·ç¦»æ›´å¯ä¿¡
            confidence_items.append(("distance", conf))
        
        # è®¡ç®—æ€»ä½“ç½®ä¿¡åº¦
        if confidence_items:
            # åŸºç¡€ç½®ä¿¡åº¦
            base_conf = sum([item[1] for item in confidence_items]) / len(confidence_items)
            
            # æ ¹æ®åŒ¹é…é¡¹ç›®æ•°é‡è°ƒæ•´
            item_count = len(confidence_items)
            if item_count >= 4:
                multiplier = 1.1  # ä¿¡æ¯ä¸°å¯Œï¼Œæé«˜ç½®ä¿¡åº¦
            elif item_count >= 3:
                multiplier = 1.05
            elif item_count == 2:
                multiplier = 0.95
            elif item_count == 1:
                multiplier = 0.85
            else:
                multiplier = 0.7
            
            confidence = min(base_conf * multiplier, 0.95)
        else:
            # æ²¡æœ‰ä»»ä½•åŒ¹é…ï¼Œæä½ç½®ä¿¡åº¦
            confidence = 0.15
        
        # æ ¹æ®æŸ¥è¯¢é•¿åº¦è°ƒæ•´ï¼ˆè¿‡çŸ­æˆ–è¿‡é•¿çš„æŸ¥è¯¢é™ä½ç½®ä¿¡åº¦ï¼‰
        query_len = len(query)
        if query_len < 5:
            confidence *= 0.7  # æŸ¥è¯¢å¤ªçŸ­
        elif query_len > 50:
            confidence *= 0.8  # æŸ¥è¯¢å¤ªé•¿
        
        parse_time = time_module.perf_counter() - start_time
        
        # æ·»åŠ åŒ¹é…è¯¦æƒ…ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        match_details = {
            "direction_matches": direction_matches,
            "color_matches": color_matches,
            "object_matches": obj_matches,
            "relation_matches": relation_matches,
            "distance_match": distance_match,
            "total_matches": len(confidence_items)
        }
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ¾„æ¸…ï¼ˆä¿¡æ¯ä¸è¶³æ—¶ï¼‰
        need_clarification = len(confidence_items) < 2 or confidence < 0.5
        clarification_question = None
        
        if need_clarification:
            if not direction and not obj:
                clarification_question = "è¯·æä¾›æ›´å¤šä¿¡æ¯ï¼šæ‚¨æ˜¯åœ¨å“ªä¸ªæ–¹å‘ï¼Œé™„è¿‘æœ‰ä»€ä¹ˆå»ºç­‘ç‰©æˆ–ç‰©ä½“ï¼Ÿ"
            elif not direction:
                clarification_question = "è¯·é—®æ‚¨åœ¨å“ªä¸ªæ–¹å‘ï¼ˆä¸œã€å—ã€è¥¿ã€åŒ—ç­‰ï¼‰ï¼Ÿ"
            elif not obj:
                clarification_question = "è¯·é—®é™„è¿‘æœ‰ä»€ä¹ˆå»ºç­‘ç‰©æˆ–ç‰©ä½“å¯ä»¥ä½œä¸ºå‚è€ƒï¼Ÿ"
            elif confidence < 0.5:
                clarification_question = "ä¿¡æ¯ä¸å¤Ÿæ˜ç¡®ï¼Œè¯·æä¾›æ›´å¤šç»†èŠ‚ï¼ˆå¦‚é¢œè‰²ã€è·ç¦»ç­‰ï¼‰ã€‚"
        
        return {
            "direction": direction,
            "color": color,
            "object": obj,
            "relation": relation,
            "distance": distance_value,
            "confidence": round(confidence, 3),
            "enhanced_used": False,
            "parse_time": parse_time,
            "real_model_used": False,  # æ˜ç¡®æ ‡è®°ä¸ºå›é€€æ¨¡å¼
            "match_details": match_details,
            "item_count": len(confidence_items),
            "need_clarification": need_clarification,
            "clarification_question": clarification_question
        }
    
    def process_query(self, request: QueryRequest, use_cache: bool = True) -> QueryResponse:
        """
        å¤„ç†æŸ¥è¯¢è¯·æ±‚
        
        Args:
            request: æŸ¥è¯¢è¯·æ±‚
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤Trueï¼‰
            
        Returns:
            æŸ¥è¯¢å“åº”
        """
        import time
        start_time = time.time()
        
        self.query_count += 1
        query_id = f"query_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{self.query_count}"
        
        # å°è¯•ä»ç¼“å­˜è·å–
        if use_cache:
            try:
                from .cache import get_cache
                cache = get_cache()
                cached_result = cache.get(
                    request.query,
                    top_k=request.top_k,
                    enable_enhanced=request.enable_enhanced
                )
                if cached_result:
                    logger.info(f"ä½¿ç”¨ç¼“å­˜ç»“æœ")
                    cached_result.query_id = query_id
                    # é‡æ–°è®¡ç®—å¤„ç†æ—¶é—´ï¼ˆä¸åŒ…æ‹¬è§£ææ—¶é—´ï¼‰
                    actual_processing_time = (time.time() - start_time) * 1000
                    cached_result.processing_time_ms = actual_processing_time
                    # æ›´æ–°è§£ææ—¶é—´ä¸ºç¼“å­˜å‘½ä¸­æ—¶é—´ï¼ˆéå¸¸å¿«ï¼‰
                    if cached_result.query_analysis:
                        cached_result.query_analysis["parse_time_ms"] = 0.1  # ç¼“å­˜å‘½ä¸­å‡ ä¹æ— è€—æ—¶
                        cached_result.query_analysis["parse_method"] = "cache_hit"
                    return cached_result
            except Exception as e:
                logger.debug(f"ç¼“å­˜è·å–å¤±è´¥: {e}")
        
        try:
            # ä½¿ç”¨å¢å¼ºæ¨¡å¼å¤„ç†ï¼ˆå³ä½¿æ²¡æœ‰adapterä¹Ÿèƒ½å·¥ä½œï¼‰
            if request.enable_enhanced:
                # å¢å¼ºæ¨¡å¼ - ä½¿ç”¨NLUè§£æ
                response = self._enhanced_process_query(request, query_id, start_time)
            else:
                # åŸå§‹æ¨¡å¼
                response = self._original_process_query(request, query_id, start_time)
            
            # ç¼“å­˜ç»“æœ
            if use_cache and response.status == "success":
                try:
                    from .cache import get_cache
                    cache = get_cache()
                    cache.set(
                        request.query,
                        response,
                        top_k=request.top_k,
                        enable_enhanced=request.enable_enhanced
                    )
                except Exception as e:
                    logger.debug(f"ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
            
            return response
                
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            return QueryResponse(
                query_id=query_id,
                status="error",
                processing_time_ms=(time.time() - start_time) * 1000,
                error=str(e)
            )
    
    def _enhanced_process_query(self, request: QueryRequest, query_id: str, start_time: float) -> QueryResponse:
        """å¢å¼ºæ¨¡å¼å¤„ç†æŸ¥è¯¢ï¼ˆä½¿ç”¨ä¼˜åŒ–ç‰ˆNLUå¼•æ“ï¼‰"""
        
        # ä½¿ç”¨ä¼˜åŒ–ç‰ˆNLUå¼•æ“è§£ææŸ¥è¯¢ï¼ˆæ”¯æŒäº¤äº’å¼ï¼‰
        query_analysis = self._parse_query_with_nlu(
            request.query, 
            session_id=request.session_id,
            interactive=request.interactive
        )
        
        # æ„å»ºæ ‡å‡†åŒ–æ ¼å¼ï¼ˆç›´æ¥åŒ…å«è§£æç»“æœï¼‰
        parse_time = query_analysis.get("parse_time", 0)
        parse_time_ms = parse_time * 1000 if parse_time else 0
        
        # è·å–æ¨¡å‹åç§°ï¼ˆä»è§£æç»“æœæˆ–é…ç½®ï¼‰
        nlu_model = query_analysis.get("nlu_model", "unknown")
        if nlu_model == "unknown":
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ DeepSeek
            if os.environ.get("DEEPSEEK_API_KEY", ""):
                nlu_model = "deepseek-chat"
            else:
                nlu_model = "qwen3-vl:2b"
        
        standard_format = {
            "original_query": request.query,
            "direction": query_analysis.get("direction"),
            "color": query_analysis.get("color"),
            "object": query_analysis.get("object"),
            "relation": query_analysis.get("relation"),
            "distance": query_analysis.get("distance"),
            "confidence": query_analysis.get("confidence", 0.8),
            "enhanced_used": query_analysis.get("enhanced_used", True),
            "nlu_model": nlu_model,
            "parse_time_ms": round(parse_time_ms, 2),
            "intent": query_analysis.get("intent"),
            "need_clarification": query_analysis.get("need_clarification", False),
            "clarification_question": query_analysis.get("clarification_question"),
            "suggestions": query_analysis.get("suggestions", []),
            "parse_method": query_analysis.get("parse_method", "unknown"),
            "real_model_used": query_analysis.get("real_model_used", False),
        }
        
        # å¦‚æœæ˜¯é—®å€™è¯­ï¼Œç›´æ¥è¿”å›
        if query_analysis.get("intent") == "greeting":
            processing_time_ms = (time.time() - start_time) * 1000
            
            response = QueryResponse(
                query_id=query_id,
                status="success",
                processing_time_ms=processing_time_ms,
                query_analysis=standard_format,
                session_id=query_analysis.get("session_id"),
                intent=query_analysis.get("intent"),
                need_clarification=query_analysis.get("need_clarification", False),
                clarification_question=query_analysis.get("clarification_question"),
                suggestions=query_analysis.get("suggestions", []),
            )
            
            if request.return_debug_info:
                response.debug_info = {
                    "nlu_engine": nlu_model,
                    "parse_method": query_analysis.get("parse_method", "unknown"),
                    "parse_time_ms": round(parse_time_ms, 2),
                    "timestamp": datetime.now().isoformat()
                }
            
            return response
        
        # ä½¿ç”¨ Text2Loc é€‚é…å™¨æŸ¥æ‰¾çœŸå®ä½ç½®
        candidates = self._find_locations_with_adapter(query_analysis, request.top_k)
        
        # æ„å»ºå“åº”
        actual_time = (time.time() - start_time) * 1000
        # å¦‚æœå®é™…æ—¶é—´è¶…è¿‡30ç§’ï¼Œæ˜¾ç¤ºè­¦å‘Šå¹¶æ ‡è®°ä¸ºè¶…æ—¶
        timed_out = actual_time > 30000
        display_time = actual_time if actual_time <= 30000 else actual_time
        
        # æ„å»ºè§£æè¯¦æƒ…
        parsing_details = ParsingDetails(
            directions=[query_analysis.get("direction")] if query_analysis.get("direction") else [],
            colors=[query_analysis.get("color")] if query_analysis.get("color") else [],
            objects=[query_analysis.get("object")] if query_analysis.get("object") else [],
            distances=[query_analysis.get("distance")] if query_analysis.get("distance") else [],
            landmarks=query_analysis.get("landmarks", [])
        )
        
        # æ„å»ºresultså­—æ®µï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
        results_list = [
            {
                "rank": i + 1,
                "cell_id": ret["cell_id"],
                "score": ret["score"],
                "method": "enhanced_nlu",
                "description": ret["description"],
                "x": ret.get("x", 0.0),
                "y": ret.get("y", 0.0),
                "confidence": ret.get("confidence", ret["score"]),
                "reference_objects": ret.get("reference_objects", [])
            }
            for i, ret in enumerate(candidates)
        ]
        
        response = QueryResponse(
            query_id=query_id,
            status="success",
            processing_time_ms=display_time,
            query_analysis=standard_format,
            parsing_details=parsing_details,
            session_id=query_analysis.get("session_id"),
            intent=query_analysis.get("intent"),
            mode="enhanced" if request.enable_enhanced else "standard",
            need_clarification=query_analysis.get("need_clarification", False),
            clarification_question=query_analysis.get("clarification_question"),
            suggestions=query_analysis.get("suggestions", []),
            retrieval_results=[
                RetrievalResultItem(
                    rank=i + 1,
                    cell_id=ret["cell_id"],
                    score=ret["score"],
                    method="enhanced_nlu",
                    description=ret["description"],
                    x=ret.get("x", 0.0),
                    y=ret.get("y", 0.0),
                    confidence=ret.get("confidence", ret["score"]),
                    reference_objects=ret.get("reference_objects", [])
                )
                for i, ret in enumerate(candidates)
            ],
            results=results_list,
            final_result=RetrievalResultItem(
                rank=1,
                cell_id=candidates[0]["cell_id"],
                score=candidates[0]["score"],
                method="enhanced_nlu",
                description=candidates[0]["description"],
                x=candidates[0].get("x", 0.0),
                y=candidates[0].get("y", 0.0),
                confidence=candidates[0].get("confidence", candidates[0]["score"]),
                reference_objects=candidates[0].get("reference_objects", [])
            ) if candidates else None
        )
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        if request.return_debug_info:
            response.debug_info = {
                "nlu_engine": nlu_model,
                "parse_method": query_analysis.get("parse_method", "unknown"),
                "parse_time_ms": round(parse_time_ms, 2),
                "api_time_ms": round(actual_time, 2),
                "timed_out": timed_out,
                "candidates_generated": len(candidates),
                "timestamp": datetime.now().isoformat()
            }
        
        return response
    
    def _generate_candidates(self, query_analysis: Dict[str, Any], top_k: int) -> List[Dict[str, Any]]:
        """
        åŸºäºè§£æç»“æœç”Ÿæˆå€™é€‰ä½ç½®
        
        Args:
            query_analysis: æŸ¥è¯¢åˆ†æç»“æœ
            top_k: è¿”å›æ•°é‡
            
        Returns:
            å€™é€‰ä½ç½®åˆ—è¡¨
        """
        import random
        
        direction = query_analysis.get("direction", "")
        color = query_analysis.get("color", "")
        obj = query_analysis.get("object", "")
        confidence = query_analysis.get("confidence", 0.5)
        item_count = query_analysis.get("item_count", 0)
        
        # åŸºäºè§£æç»“æœç”Ÿæˆæè¿°
        descriptions = []
        if obj and direction and color:
            descriptions = [
                f"{color}è‰²çš„{obj}çš„{direction}ä¾§",
                f"åœ¨{direction}è¾¹é è¿‘{color}{obj}",
                f"{color}{obj}é™„è¿‘ï¼Œæ–¹å‘{direction}"
            ]
        elif obj and direction:
            descriptions = [
                f"{obj}çš„{direction}ä¾§",
                f"{direction}æ–¹å‘çš„{obj}é™„è¿‘",
                f"é è¿‘{direction}è¾¹çš„{obj}"
            ]
        elif obj and color:
            descriptions = [
                f"{color}è‰²çš„{obj}é™„è¿‘",
                f"åœ¨{obj}æ—è¾¹ï¼Œé¢œè‰²ä¸º{color}"
            ]
        elif obj:
            descriptions = [
                f"{obj}é™„è¿‘",
                f"é è¿‘{obj}çš„ä½ç½®",
                f"{obj}å‘¨å›´"
            ]
        else:
            descriptions = [
                "å€™é€‰ä½ç½®1",
                "å€™é€‰ä½ç½®2",
                "å€™é€‰ä½ç½®3"
            ]
        
        # ç”Ÿæˆå€™é€‰ç»“æœ - ä½¿ç”¨çœŸå®éšæœºæ•°
        candidates = []
        
        # åŸºäºç½®ä¿¡åº¦å’Œä¿¡æ¯ä¸°å¯Œåº¦è®¡ç®—æœ€ä½³åˆ†æ•°
        base_score = confidence
        
        # ä¿¡æ¯è¶Šä¸°å¯Œï¼Œåˆ†æ•°è¶Šé«˜
        if item_count >= 4:
            base_score *= 1.1
        elif item_count >= 3:
            base_score *= 1.05
        elif item_count == 1:
            base_score *= 0.9
        
        base_score = min(base_score, 0.95)
        
        for i, desc in enumerate(descriptions[:top_k]):
            # æ¯ä¸ªå€™é€‰ä½ç½®çš„åˆ†æ•°ç•¥æœ‰ä¸åŒï¼Œä½¿ç”¨éšæœºæ•°å¢åŠ çœŸå®æ„Ÿ
            # åˆ†æ•°é€’å‡ï¼Œä½†åŸºäºçœŸå®éšæœºæ•°
            offset = random.uniform(0.05, 0.15) * (i + 0.5)
            score = base_score - offset
            
            # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
            score = max(score, 0.5)
            score = min(score, 0.95)
            
            # ç”Ÿæˆ2Dåæ ‡ï¼ˆåŸºäºæ–¹å‘åç§»ï¼‰
            base_x, base_y = 100.0, 100.0
            direction_offsets = {
                "north": (0, 10),
                "south": (0, -10),
                "east": (10, 0),
                "west": (-10, 0),
                "northeast": (7, 7),
                "northwest": (-7, 7),
                "southeast": (7, -7),
                "southwest": (-7, -7),
            }
            dx, dy = direction_offsets.get(direction, (random.uniform(-10, 10), random.uniform(-10, 10)))
            x = base_x + dx + random.uniform(-5, 5)
            y = base_y + dy + random.uniform(-5, 5)
            
            candidates.append({
                "cell_id": f"cell_{i:03d}",
                "score": round(score, 3),
                "description": desc,
                "x": round(x, 2),
                "y": round(y, 2),
                "confidence": round(score, 3),
                "reference_objects": [obj] if obj else []
            })
        
        # æŒ‰åˆ†æ•°æ’åºï¼ˆé™åºï¼‰
        candidates.sort(key=lambda x: x["score"], reverse=True)
        
        return candidates
    
    def _find_locations_with_adapter(self, query_analysis: Dict[str, Any], top_k: int) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ Text2Loc é€‚é…å™¨æŸ¥æ‰¾çœŸå®ä½ç½®
        
        Args:
            query_analysis: æŸ¥è¯¢åˆ†æç»“æœ
            top_k: è¿”å›æ•°é‡
            
        Returns:
            å€™é€‰ä½ç½®åˆ—è¡¨ï¼ˆåŒ…å«çœŸå®åæ ‡ï¼‰
        """
        direction = query_analysis.get("direction", "")
        color = query_analysis.get("color", "")
        obj = query_analysis.get("object", "")
        query = query_analysis.get("original_query", "")
        
        logger.info(f"ğŸ” ä½¿ç”¨ Text2Loc é€‚é…å™¨æŸ¥æ‰¾ä½ç½®:")
        logger.info(f"   æŸ¥è¯¢: {query}")
        logger.info(f"   æ–¹å‘: {direction}, é¢œè‰²: {color}, å¯¹è±¡: {obj}")
        
        # ä½¿ç”¨é€‚é…å™¨æŸ¥æ‰¾ä½ç½®
        if self.adapter:
            candidates = self.adapter.find_location(
                query=query,
                direction=direction,
                color=color,
                obj=obj,
                top_k=top_k
            )
            logger.info(f"   æ‰¾åˆ° {len(candidates)} ä¸ªå€™é€‰ä½ç½®")
            return candidates
        else:
            logger.warning("âš ï¸ é€‚é…å™¨æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            return self._generate_candidates(query_analysis, top_k)
        
    def _original_process_query(self, request: QueryRequest, query_id: str, start_time: float) -> QueryResponse:
        """åŸå§‹æ¨¡å¼å¤„ç†æŸ¥è¯¢"""
        processing_time_ms = (time.time() - start_time) * 1000
        
        return QueryResponse(
            query_id=query_id,
            status="success",
            processing_time_ms=processing_time_ms,
            query_analysis={"mode": "original"},
            retrieval_results=[
                RetrievalResultItem(
                    rank=1,
                    cell_id="cell_000",
                    score=0.9,
                    method="template",
                    description="å€™é€‰æè¿°",
                    x=100.50,
                    y=200.75,
                    confidence=0.9,
                    reference_objects=["å»ºç­‘ç‰©A", "é“è·¯B"]
                )
            ],
            final_result=RetrievalResultItem(
                rank=1,
                cell_id="cell_000",
                score=0.9,
                method="template",
                description="å€™é€‰æè¿°",
                x=100.50,
                y=200.75,
                confidence=0.9,
                reference_objects=["å»ºç­‘ç‰©A", "é“è·¯B"]
            )
        )
    
    def _mock_process_query(self, request: QueryRequest, query_id: str, start_time: float) -> QueryResponse:
        """æ¨¡æ‹Ÿæ¨¡å¼å¤„ç†æŸ¥è¯¢"""
        import random
        
        processing_time_ms = random.uniform(10, 100)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        
        # ç”Ÿæˆæ¨¡æ‹Ÿåæ ‡
        base_x, base_y = 100.0, 100.0
        
        return QueryResponse(
            query_id=query_id,
            status="success",
            processing_time_ms=processing_time_ms,
            query_analysis={
                "original_query": request.query,
                "mode": "mock",
                "direction": "north",
                "confidence": 0.85
            },
            retrieval_results=[
                RetrievalResultItem(
                    rank=i + 1,
                    cell_id=f"cell_{i:03d}",
                    score=0.9 - (i * 0.1),
                    method="mock",
                    description=f"Mock result {i + 1}",
                    x=base_x + i * 10.5,
                    y=base_y + i * 8.3,
                    confidence=0.9 - (i * 0.1),
                    reference_objects=[f"å‚è€ƒå¯¹è±¡{i+1}A", f"å‚è€ƒå¯¹è±¡{i+1}B"]
                )
                for i in range(min(request.top_k, 5))
            ],
            final_result=RetrievalResultItem(
                rank=1,
                cell_id="cell_000",
                score=0.9,
                method="mock",
                description="Mock result 1",
                x=base_x,
                y=base_y,
                confidence=0.9,
                reference_objects=["å‚è€ƒå¯¹è±¡1A", "å‚è€ƒå¯¹è±¡1B"]
            )
        )
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–APIçŠ¶æ€"""
        return {
            "status": "running",
            "query_count": self.query_count,
            "adapter_available": self.adapter is not None,
            "timestamp": datetime.now().isoformat()
        }
    
    def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        return {
            "status": "healthy",
            "components": {
                "api": "ok",
                "adapter": "ok" if self.adapter else "mock",
                "config": "ok"
            }
        }


def create_api(adapter=None, config_path=None) -> Text2LocAPI:
    """
    åˆ›å»ºAPIå®ä¾‹
    
    Args:
        adapter: Text2Locé€‚é…å™¨
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        Text2LocAPIå®ä¾‹
    """
    # åŠ è½½é…ç½®
    config = None
    if config_path:
        try:
            from enhancements.integration.config_manager import ConfigManager
            config_manager = ConfigManager(config_path)
            config = config_manager.config
        except Exception as e:
            logger.warning(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
    
    # åˆ›å»ºAPI
    api = Text2LocAPI(adapter=adapter, config=config)
    
    # è®¾ç½®é€‚é…å™¨
    if adapter:
        api.set_adapter(adapter)
    
    return api
